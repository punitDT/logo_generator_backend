# ğŸ¨ AI Logo Generator - GPU Edition

**Production-ready FastAPI backend for AI logo generation using local GPU inference with Flux model.**

This is a complete refactor of the original HuggingFace API-based backend to run **entirely on GPU** using local model inference, optimized for deployment on **Vast.ai** or any CUDA-enabled GPU machine.

---

## âœ¨ Key Features

- **ğŸš€ Local GPU Inference**: Runs Flux model directly on GPU (no external API calls)
- **âš¡ High Performance**: FP16 precision, attention slicing, concurrent request handling
- **ğŸ”„ Production Ready**: Async FastAPI, JSON logging, health monitoring, exception handling
- **ğŸ¯ Optimized for Vast.ai**: Docker container with CUDA support, automatic GPU warmup
- **ğŸ“Š Monitoring**: GPU stats, request tracking, detailed logging
- **ğŸ”§ Configurable**: Environment-based configuration for all parameters
- **ğŸ’° Cost Effective**: Run on affordable GPU instances (~$0.20-0.60/hour)

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FastAPI Server                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Routes     â”‚  â”‚  Middleware  â”‚  â”‚   Logging    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Model Manager (Singleton)               â”‚
â”‚  â€¢ Load Flux model once at startup                      â”‚
â”‚  â€¢ Manage GPU memory and concurrency                    â”‚
â”‚  â€¢ Async inference with semaphore control               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GPU (CUDA)                            â”‚
â”‚  â€¢ Flux.1-dev model in FP16                             â”‚
â”‚  â€¢ Attention slicing for memory optimization            â”‚
â”‚  â€¢ Concurrent inference (configurable)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ What's New in GPU Edition

### Removed
- âŒ HuggingFace Inference API dependency
- âŒ External API calls
- âŒ API token requirements (for inference)

### Added
- âœ… Local GPU model loading with `diffusers`
- âœ… Model manager with singleton pattern
- âœ… Async inference with concurrency control
- âœ… GPU warmup on startup
- âœ… JSON structured logging
- âœ… Health endpoint with GPU stats
- âœ… Exception handling middleware
- âœ… CUDA-compatible Dockerfile
- âœ… Vast.ai deployment guide

---

## ğŸš€ Quick Start

### Prerequisites
- **GPU**: NVIDIA GPU with 24GB+ VRAM (RTX 3090, 4090, A6000, etc.)
- **CUDA**: Version 12.1+
- **Docker**: With NVIDIA Container Toolkit
- **OR** Python 3.10+ with CUDA support

### Option 1: Docker (Recommended)

```bash
# Clone repository
git clone https://github.com/punitDT/logo_generator_backend.git
cd logo_generator_backend

# Create environment file
cp .env.example .env

# Build image
docker build -t logo-generator-gpu .

# Run container
docker run -d \
  --name logo-generator \
  --gpus all \
  -p 7860:7860 \
  -v $(pwd)/.env:/app/.env \
  -v /root/.cache/huggingface:/root/.cache/huggingface \
  logo-generator-gpu

# Check logs
docker logs -f logo-generator
```

### Option 2: Direct Python

```bash
# Clone repository
git clone https://github.com/punitDT/logo_generator_backend.git
cd logo_generator_backend

# Install PyTorch with CUDA
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121

# Install dependencies
pip install -r requirements.txt

# Create environment file
cp .env.example .env

# Run server
python run.py
```

---

## ğŸ“š API Documentation

### Endpoints

#### `GET /health`
Health check with GPU status

**Response:**
```json
{
  "status": "healthy",
  "model": "black-forest-labs/FLUX.1-dev",
  "model_loaded": true,
  "gpu": {
    "gpu_available": true,
    "device_name": "NVIDIA RTX 4090",
    "memory_allocated_gb": 15.2,
    "memory_total_gb": 24.0
  }
}
```

#### `POST /api/generate_logo`
Generate professional logos

**Request:**
```json
{
  "prompt": "Modern tech startup logo",
  "sizes": [256, 512, 1024]
}
```

**Response:**
```json
{
  "message": "Logo generated successfully!",
  "prompt": "Modern tech startup logo",
  "images": {
    "256": "base64_encoded_image...",
    "512": "base64_encoded_image...",
    "1024": "base64_encoded_image..."
  },
  "sizes": [256, 512, 1024],
  "model": "black-forest-labs/FLUX.1-dev"
}
```

---

## âš™ï¸ Configuration

Key environment variables (see `.env.example` for all options):

```bash
# Model
MODEL_NAME=black-forest-labs/FLUX.1-dev
USE_FP16=true

# GPU
MAX_CONCURRENT_JOBS=2
REQUEST_TIMEOUT=300

# Inference
DEFAULT_INFERENCE_STEPS=28
DEFAULT_GUIDANCE_SCALE=3.5

# Logging
LOG_LEVEL=INFO
LOG_FORMAT=json
```

---

## ğŸŒ Vast.ai Deployment

See **[VASTAI_DEPLOYMENT.md](VASTAI_DEPLOYMENT.md)** for complete deployment guide.

**Quick Deploy:**
1. Rent GPU on Vast.ai (RTX 4090 recommended)
2. SSH into instance
3. Clone repo and run Docker container
4. Access API at `http://<VAST_IP>:<VAST_PORT>`

**Estimated Costs:**
- RTX 3090: $0.20-0.40/hour
- RTX 4090: $0.40-0.60/hour
- A6000: $0.60-1.00/hour

---

## ğŸ“Š Performance

**RTX 4090 Benchmarks:**
- 512x512 @ 28 steps: ~12 seconds
- 1024x1024 @ 28 steps: ~18 seconds
- Concurrent (2 jobs): ~25 seconds total

---

## ğŸ—‚ï¸ Project Structure

```
logo_generator_backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                 # FastAPI app with lifespan events
â”‚   â”œâ”€â”€ config.py               # GPU configuration
â”‚   â”œâ”€â”€ model_manager.py        # Singleton model manager
â”‚   â”œâ”€â”€ logging_config.py       # JSON/text logging
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â””â”€â”€ generate_logo_routes.py
â”‚   â””â”€â”€ services/
â”‚       â””â”€â”€ generate_logo_service.py  # GPU inference logic
â”œâ”€â”€ run.py                      # Entry point with GPU checks
â”œâ”€â”€ requirements.txt            # GPU dependencies
â”œâ”€â”€ Dockerfile                  # CUDA-compatible image
â”œâ”€â”€ .env.example               # Configuration template
â”œâ”€â”€ VASTAI_DEPLOYMENT.md       # Deployment guide
â””â”€â”€ README_GPU.md              # This file
```

---

## ğŸ”§ Development

```bash
# Install in development mode
pip install -r requirements.txt

# Run with auto-reload
ENVIRONMENT=development python run.py

# View logs in text format
LOG_FORMAT=text python run.py

# Access API docs
http://localhost:7860/docs
```

---

## ğŸ› Troubleshooting

See [VASTAI_DEPLOYMENT.md](VASTAI_DEPLOYMENT.md#-monitoring--troubleshooting) for detailed troubleshooting.

**Common Issues:**
- **CUDA Out of Memory**: Reduce `MAX_CONCURRENT_JOBS` or `DEFAULT_INFERENCE_STEPS`
- **Slow First Request**: Model loading takes 2-5 minutes (enable warmup)
- **Port Not Accessible**: Check Vast.ai port forwarding

---

## ğŸ“„ License

MIT License - See LICENSE file for details

---

## ğŸ™ Acknowledgments

- **Flux Model**: [Black Forest Labs](https://huggingface.co/black-forest-labs/FLUX.1-dev)
- **Diffusers**: [HuggingFace Diffusers](https://github.com/huggingface/diffusers)
- **FastAPI**: [Tiangolo](https://fastapi.tiangolo.com)

---

**ğŸš€ Ready to deploy? Check out [VASTAI_DEPLOYMENT.md](VASTAI_DEPLOYMENT.md)!**

