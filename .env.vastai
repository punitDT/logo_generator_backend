
# ============================================
# AI Logo Generator - Vast.ai Production Configuration
# For GPU Deployment (NVIDIA CUDA)
# ============================================

# ----------------
# Server Settings
# ----------------
PORT=7860
HOST=0.0.0.0
ENVIRONMENT=production

# ----------------
# Model Configuration
# ----------------
# HuggingFace model name (will be downloaded on first run)
MODEL_NAME=black-forest-labs/FLUX.1-dev

# OR use local model path (if you have pre-downloaded the model)
# MODEL_PATH=/app/models/flux-dev

# Use FP16 for faster inference on NVIDIA GPUs
USE_FP16=true

# ----------------
# GPU Configuration
# ----------------
# Auto-detect device (will use CUDA on Vast.ai)
TORCH_DEVICE=auto
ENABLE_ATTENTION_SLICING=true
ENABLE_VAE_SLICING=true

# ----------------
# Inference Settings
# ----------------
# Maximum number of concurrent inference jobs
# Adjust based on GPU VRAM:
# - RTX 3090 (24GB): 1-2
# - RTX 4090 (24GB): 2-3
# - A6000 (48GB): 3-4
# - A100 (40GB): 3-5
MAX_CONCURRENT_JOBS=2

# Request timeout in seconds
REQUEST_TIMEOUT=300

# Default inference parameters
DEFAULT_INFERENCE_STEPS=28
DEFAULT_GUIDANCE_SCALE=3.5

# ----------------
# Warmup Configuration
# ----------------
# Enable warmup for production (warms up GPU on startup)
WARMUP_ENABLED=true
WARMUP_PROMPT=test logo
WARMUP_SIZE=512
WARMUP_STEPS=10

# ----------------
# Logging Configuration
# ----------------
# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Log format: json (for production) or text (for development)
LOG_FORMAT=json

# ----------------
# HuggingFace Token (Optional)
# ----------------
# Only needed if downloading gated models
# Get your token from: https://huggingface.co/settings/tokens
HF_TOKEN=

# ----------------
# Vast.ai Configuration
# ----------------
# Your Vast.ai API key (optional, for programmatic access)
VAST_AI_API_KEY=

